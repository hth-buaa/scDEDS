% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_evaluate.R
\name{model_evaluate}
\alias{model_evaluate}
\title{Comprehensive Binary Classification Model Evaluation}
\usage{
model_evaluate(pred_vec, stand_vec)
}
\arguments{
\item{pred_vec}{Numeric vector. Predicted binary labels (0 or 1) from a classification model. This vector has the same dimension as stand_vec.}

\item{stand_vec}{Numeric vector. Actual ground truth binary labels (0 or 1). This vector has the same dimension as pred_vec}
}
\value{
A named numeric vector containing 15 performance metrics:
\itemize{
\item \code{TP}: True Positives
\item \code{TN}: True Negatives
\item \code{FP}: False Positives
\item \code{FN}: False Negatives
\item \code{Accuracy}: Overall accuracy (TP+TN)/(TP+TN+FP+FN)
\item \code{Recall}: Sensitivity/True Positive Rate (TP/(TP+FN))
\item \code{Precision}: Positive Predictive Value (TP/(TP+FP))
\item \code{Specificity}: True Negative Rate (TN/(TN+FP))
\item \code{NegPredRate}: Negative Predictive Value (TN/(TN+FN))
\item \code{Kappa}: Cohen's Kappa statistic (agreement beyond chance)
\item \code{F1}: F1 Score (harmonic mean of precision and recall)
\item \code{Prevalence}: Proportion of actual positives in dataset
\item \code{DetectionRate}: Proportion of correctly detected positives
\item \code{BalancedAccuracy}: Average of recall and specificity
\item \code{auc}: Area Under the ROC Curve
}
}
\description{
This function performs a comprehensive evaluation of binary classification models by computing multiple performance metrics from predicted and actual labels.
It calculates standard classification metrics, advanced statistical measures, and ROC analysis to provide a holistic view of model performance.
}
\details{
The function implements the following evaluation methodology:
\enumerate{
\item Confusion Matrix: Computes the confusion matrix to get TP, TN, FP, FN counts
\item Basic Metrics: Calculates accuracy, recall, precision, specificity, and NPV
\item Advanced Metrics: Computes Cohen's Kappa, F1 score, prevalence, detection rate, and balanced accuracy
\item ROC Analysis: Uses the \code{pROC} package to calculate AUC
}
All metrics include a small epsilon (1e-8) in denominators to prevent division by zero.
This ensures numerical stability while maintaining metric accuracy.
}
\note{
Important considerations:
\itemize{
\item Input vectors must contain only binary values (0 and 1)
\item The function assumes 0 represents the negative class and 1 the positive class
\item Kappa values range from -1 (worse than random) to 1 (perfect agreement)
\item AUC values range from 0 (uncommon and worse than random guessing) to 1 (perfect classifier), with 0.5 representing a random classifier
\item Balanced accuracy is particularly useful for imbalanced datasets
\item The function requires the \code{pROC} package for AUC calculation
}
}
\examples{
\dontrun{
# Create sample predicted and actual labels
predictions <- c(1, 0, 1, 1, 0, 1, 0, 0)
actuals <- c(1, 0, 0, 1, 0, 1, 1, 0)
# Evaluate model performance
results <- model_evaluate(pred_vec = predictions, stand_vec = actuals)
}
}
\seealso{
\code{\link[pROC]{roc}}, \code{\link[base]{table}}
}
